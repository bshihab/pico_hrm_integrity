{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586d1daa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m     11\u001b[39m load_dotenv()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# config\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import os\n",
    "import requests\n",
    "import urllib.parse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# config\n",
    "INPUT_SIZE = 10\n",
    "HIDDEN_SIZE = 8\n",
    "CLASSES = 3\n",
    "CSV_FILENAME = 'mitbih_test.csv'\n",
    "\n",
    "# google cloud config\n",
    "GCP_API_KEY = os.getenv(\"GCP_API_KEY\") \n",
    "GCP_BUCKET_NAME = \"heart-data-repo-1\" \n",
    "GCP_OBJECT_NAME = \"mitbih_test.csv\"\n",
    "\n",
    "def get_google_cloud_url():\n",
    "    \"\"\"Constructs the authenticated Google Cloud Storage API URL.\"\"\"\n",
    "    encoded_name = urllib.parse.quote(GCP_OBJECT_NAME, safe='')\n",
    "    url = f\"https://storage.googleapis.com/storage/v1/b/{GCP_BUCKET_NAME}/o/{encoded_name}?alt=media&key={GCP_API_KEY}\"\n",
    "    return url\n",
    "\n",
    "def force_download_data(): \n",
    "    # to make sure that if the file is not available locally that we can download it on google cloud\n",
    "    if os.path.exists(CSV_FILENAME) and os.path.getsize(CSV_FILENAME) > 1000:\n",
    "        print(f\"Data present: {CSV_FILENAME}\")\n",
    "        return\n",
    "\n",
    "    target_url = get_google_cloud_url()\n",
    "    print(f\"Downloading data from Google Cloud Storage...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(target_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(CSV_FILENAME, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Download successful!\")\n",
    "        else:\n",
    "            print(f\"Download failed (Status: {response.status_code})\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Network error: {e}\")\n",
    "\n",
    "def load_data():\n",
    "    force_download_data()\n",
    "    \n",
    "    if not os.path.exists(CSV_FILENAME):\n",
    "        print(\"Error, File not found.\")\n",
    "        return None, None\n",
    "        \n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(CSV_FILENAME, header=None)\n",
    "    \n",
    "    # only keep roles that deal with normal (0), s-type (1), and v-type (2) \n",
    "    df = df[df[187].isin([0.0, 1.0, 2.0])] \n",
    "    \n",
    "    # Train on FIRST 16,000 samples\n",
    "    X = df.iloc[:16000, 20:20+INPUT_SIZE].values\n",
    "    # Slice y to match X length\n",
    "    y = df.iloc[:16000, 187].values.astype(int)\n",
    "    \n",
    "    # one hot encoding, converting column numbers to vectors  \n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=CLASSES) \n",
    "    return X, y\n",
    "\n",
    "def quantize_and_export_manual(model):\n",
    "    #  LAYER 1 (Hidden)\n",
    "    weights1, biases1 = model.layers[0].get_weights()\n",
    "    \n",
    "    w1_max = np.max(np.abs(weights1))\n",
    "    # maps largest weight to maximum 8-bit integer value\n",
    "    w1_scale = w1_max / 127.0 \n",
    "    \n",
    "    # scale up floats to 8 bit integers by dividing by scale and truncating\n",
    "    w1_int = (weights1 / w1_scale).astype(np.int8) \n",
    "    \n",
    "    #  LAYER 2 (Output) \n",
    "    weights2, biases2 = model.layers[1].get_weights() \n",
    "    \n",
    "    w2_max = np.max(np.abs(weights2))\n",
    "    # different scale than w1 because the max is different\n",
    "    w2_scale = w2_max / 127.0 \n",
    "    w2_int = (weights2 / w2_scale).astype(np.int8)\n",
    "    \n",
    "    #  GENERATE C HEADER CONTENT\n",
    "    c_code = \"// Auto-generated by train_tinyml.py\\n\"\n",
    "    c_code += \"#ifndef MODEL_WEIGHTS_H\\n#define MODEL_WEIGHTS_H\\n\\n\"\n",
    "    c_code += \"#include <stdint.h>\\n\\n\"\n",
    "\n",
    "    c_code += f\"// Layer 1 (Hidden) - Explicit Quantization\\n\"\n",
    "    c_code += f\"const float W1_SCALE = {w1_scale:.12f};\\n\"\n",
    "    c_code += f\"const int8_t W1[{INPUT_SIZE}][{HIDDEN_SIZE}] = {{\\n\"\n",
    "    for i in range(INPUT_SIZE):\n",
    "        row = \", \".join([f\"{w:4d}\" for w in w1_int[i]])\n",
    "        c_code += f\"    {{{row}}},\\n\"\n",
    "    c_code += \"};\\n\"\n",
    "    c_code += f\"const float B1[{HIDDEN_SIZE}] = {{ \" + \", \".join([f\"{b:.6f}\" for b in biases1]) + \" };\\n\\n\"\n",
    "\n",
    "    c_code += f\"// Layer 2 (Output) - Explicit Quantization\\n\"\n",
    "    c_code += f\"const float W2_SCALE = {w2_scale:.12f};\\n\"\n",
    "    # must print 8-bit int for pico to run more efficiently, but pico will scale it back up before running inference.\n",
    "    c_code += f\"const int8_t W2[{HIDDEN_SIZE}][{CLASSES}] = {{\\n\"\n",
    "    for i in range(HIDDEN_SIZE):\n",
    "        row = \", \".join([f\"{w:4d}\" for w in w2_int[i]])\n",
    "        c_code += f\"    {{{row}}},\\n\"\n",
    "    c_code += \"};\\n\"\n",
    "    c_code += f\"const float B2[{CLASSES}] = {{ \" + \", \".join([f\"{b:.6f}\" for b in biases2]) + \" };\\n\"\n",
    "    \n",
    "    c_code += \"\\n#endif // MODEL_WEIGHTS_H\\n\"\n",
    "\n",
    "    #  SAVE TO FILE (ROBUST PATH FINDING) \n",
    "    possible_paths = [\n",
    "        \"../firmware/src/model_weights.h\",        # If running from training/ folder\n",
    "        \"firmware/src/model_weights.h\",           # If running from root folder\n",
    "        \"src/model_weights.h\",                    # If running from inside firmware/\n",
    "        \"/content/model_weights.h\"                # Google Colab Default\n",
    "    ]\n",
    "\n",
    "    saved = False\n",
    "    for path in possible_paths:\n",
    "        dir_name = os.path.dirname(path)\n",
    "        if os.path.exists(dir_name) or dir_name == \"/content\":\n",
    "            try:\n",
    "                with open(path, \"w\") as f:\n",
    "                    f.write(c_code)\n",
    "                print(f\"SUCCESS: Auto-updated header file at: {os.path.abspath(path)}\")\n",
    "                print(\"You can now Build the firmware immediately!\")\n",
    "                saved = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                pass \n",
    "\n",
    "    if not saved:\n",
    "        filename = \"model_weights.h\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(c_code)\n",
    "        print(f\"WARNING: Could not find firmware folder automatically.\")\n",
    "        print(f\"Saved '{filename}' to current directory.\")\n",
    "        print(\"ACTION: Please move this file into 'firmware/src/' manually.\")\n",
    "\n",
    "def main():\n",
    "    X, y = load_data() \n",
    "    if X is None: return\n",
    "\n",
    "    print(f\"Training on {len(X)} samples...\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(HIDDEN_SIZE, input_dim=INPUT_SIZE, activation='relu'),\n",
    "        # converting scores to probabilities\n",
    "        Dense(CLASSES, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    # we use adam optimizer becayse idk?\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    loss, acc = model.evaluate(X, y, verbose=1)\n",
    "    print(f\"Model Accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    # Run the automated export\n",
    "    quantize_and_export_manual(model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
